Загрузка необходимых пакетов

```{r libraries, message=FALSE}
require(tidyverse)
require(Quandl)
```

# №1

Загрузка данных

```{r}
d <- as_tibble(Quandl("WIKI/GOOGL"))
d <- d %>% arrange(Close)
```

Регрессия для Close от всех возможных параметров(возьмем первые 75% записей)

```{r}
trainData <- d %>%
    select(1:8) %>%
    slice(1:(9*nrow(.) %/% 10))

testData <-  d %>%
    select(1:8) %>%
    slice(((9*nrow(.) %/% 10) + 1):nrow(.))
```

```{r}
fit <- lm(Close ~ . ,data = trainData)
summary(fit)
```

Для параметра Close статистически значимо влияют все параметры кроме Volume, так что можно убрать его

```{r}
fit <- lm(Close ~ . - Volume,data = trainData)
summary(fit)
```

Без Volume не влияет параметр Date

```{r}
fit <- lm(Close ~ . - Volume - Date,data = trainData)
summary(fit)
```

Получилась некоторая модель в которой каждый параметр влияет на Close

```{r}
predicted <- predict(fit,newdata = testData)
predicted <- as_tibble(predicted)
predicted$pos <- 1:nrow(predicted)
testData$pos <- 1:nrow(testData)

ggplot(predicted,aes(x = pos, y = value))+
    geom_line(color = "red")

ggplot(testData,aes(x = pos, y = Close))+
    geom_line(color = "blue")

ggplot()+
    geom_line(data = predicted,aes(x = pos, y = value),color = "red")+
    geom_line(data = testData,aes(x = pos, y = Close),color = "blue")
```

Если увеличить тестовые данные и уменьшить тренировочные до соотношения 1:1

```{r}
trainData <- d %>%
    select(1:8) %>%
    slice(1:(2*nrow(.) %/% 4))

testData <-  d %>%
    select(1:8) %>%
    slice(((2*nrow(.) %/% 4) + 1):nrow(.))
```

Новая модель

```{r}
fit <- lm(Close ~ . - Volume - Date,data = trainData)
summary(fit)
```

```{r}
predicted <- predict(fit,newdata = testData)
predicted <- as_tibble(predicted)
predicted$pos <- 1:nrow(predicted)
testData$pos <- 1:nrow(testData)

ggplot(predicted,aes(x = pos, y = value))+
    geom_line(color = "red")

ggplot(testData,aes(x = pos, y = Close))+
    geom_line(color = "blue")

ggplot()+
    geom_line(data = predicted,aes(x = pos, y = value),color = "red")+
    geom_line(data = testData,aes(x = pos, y = Close),color = "blue")
```

По полученным графикам видно, что возможено некоторое переобучение, т.к. график предсказанных значений
довольно близок к реальному, а шумы не очень сильные.

# №2
В папке со скриптом на R находится папка Task с дополнительными датасетами. Загрузим второй датасет и
посмотрим график зависмости

```{r,message = FALSE}
d <- read_csv("./Task/challenge_dataset.txt",col_names = F)
```

```{r}
head(d, n = 5)
```

```{r}
ggplot(d,aes(X1,X2, color = X1))+
    geom_point()+
    scale_color_gradient(low = "#0a5fd6",high = "#a900f2")
```

Для этих данных построим простую линейную регрессию x2 от x1

```{r}
fit <- lm(X2 ~ X1, data = d)
summary(fit)
ggplot(d,aes(X1,X2, color = X1))+
    geom_point()+
    scale_color_gradient(low = "#0a5fd6",high = "#a900f2")+
    geom_smooth(method = lm,formula = y ~ x)
```

Полученная модель может хорошо предсказать данные для которых x1 > 10,
но в отрезке от 5 до 10 получился слишком большой разброс.

Т.к. зависимость очень x2 от x2 напоминает график функции корня и логарифма,
построим регрессию для этих двух преобразований

```{r}
fit <- lm(X2 ~ sqrt(X1), data = d)
summary(fit)
ggplot(d,aes(X1,X2, color = X1))+
    geom_point()+
    scale_color_gradient(low = "#0a5fd6",high = "#a900f2")+
    geom_smooth(method = lm,formula = y ~ sqrt(x))
```
```{r}
fit <- lm(X2 ~ log(X1), data = d)
summary(fit)
ggplot(d,aes(X1,X2, color = X1))+
    geom_point()+
    scale_color_gradient(low = "#0a5fd6",high = "#a900f2")+
    geom_smooth(method = lm, formula = y ~ log(x))
```

Но все равно эти модели не очень хорошо описывают данные, можно и получше.
Вот так для примера будет строиться модель(не регрессия)
для этих данных по умолчанию(функция loess - LOcal regrESSion)

```{r}
ggplot(d,aes(X1,X2, color = X1))+
    geom_point()+
    scale_color_gradient(low = "#0a5fd6",high = "#a900f2")+
    geom_smooth()
```

У этой модели большие доверительные интервалы и покрывает она данные лучше предыдущих.

# №3

Загрузка данных

```{r, message=FALSE}
d <- read_csv("./Task/global_co2.csv")
```

Будем предсказать параметр 'Per capita'
Т.к. в этом параметре есть NA, то необходимо сначала отчистить данные 

```{r}
clean <- function(d, replace = 0){
    d %>% mutate_if(~ any(is.na(.)),~ ifelse(is.na(.),replace,.))
}

d <- clean(d,0)
```

Разделение данных на два множества: для обучения и для тестирования

```{r}
trainData <- d %>%
    slice(1:(3*nrow(d) %/% 4))
testData <- d %>%
    slice(((3*nrow(d) %/% 4)+1):nrow(d))
```

```{r}
fit <- lm(`Per Capita` ~ .,data = d)
summary(fit)
```

Видно, что в данной модели важдый только два параметра Year и смещение.
Для полученной модели построим предсказанные зачения.

```{r}
predict <- as_tibble(predict(fit,newdata = testData))
predict$pos <- 1:nrow(predict)
testData$pos <- 1:nrow(testData)
ggplot()+
    geom_line(data = predict,aes(x = pos , y = value),color = "red")+
    geom_line(data = testData,aes(x = pos, y = `Per Capita`), color = "blue")
```

В данной модели переобучения не видно.
Так выглядит модель если изменить соотношение данных для обучения и тестирования как 1:1

```{r}
trainData <- d %>%
    slice(1:(nrow(d) %/% 2))
testData <- d %>%
    slice(((nrow(d) %/% 2)+1):nrow(d))
```

```{r}
fit <- lm(`Per Capita` ~ .,data = d)
summary(fit)
```

```{r}
predict <- as_tibble(predict(fit,newdata = testData))
predict$pos <- 1:nrow(predict)
testData$pos <- 1:nrow(testData)

ggplot()+
    geom_line(data = predict,aes(x = pos , y = value),color = "red")+
    geom_line(data = testData,aes(x = pos, y = `Per Capita`), color = "blue")
```


Так же можно заполнять неизвестные значения другим способом.
Например средним от известных значений

```{r, message=FALSE}
d <- read_csv("./Task/global_co2.csv")
```

```{r}
d <- clean(d,mean(d$`Per Capita`,na.rm = T))
```

Разделение данных на два множества: для обучения и для тестирования

```{r}
trainData <- d %>%
    slice(1:(3*nrow(d) %/% 4))
testData <- d %>%
    slice(((3*nrow(d) %/% 4)+1):nrow(d))
```

```{r}
fit <- lm(`Per Capita` ~ .,data = d)
summary(fit)
```

```{r}
predict <- as_tibble(predict(fit,newdata = testData))
predict$pos <- 1:nrow(predict)
testData$pos <- 1:nrow(testData)
ggplot()+
    geom_line(data = predict,aes(x = pos , y = value),color = "red")+
    geom_line(data = testData,aes(x = pos, y = `Per Capita`), color = "blue")
```

В данной модели переобучения не видно.
Так выглядит модель если изменить соотношение данных для обучения и тестирования как 1:1

```{r}
trainData <- d %>%
    slice(1:(nrow(d) %/% 2))
testData <- d %>%
    slice(((nrow(d) %/% 2)+1):nrow(d))
```

```{r}
fit <- lm(`Per Capita` ~ .,data = d)
summary(fit)
```

```{r}
predict <- as_tibble(predict(fit,newdata = testData))
predict$pos <- 1:nrow(predict)
testData$pos <- 1:nrow(testData)

ggplot()+
    geom_line(data = predict,aes(x = pos , y = value),color = "red")+
    geom_line(data = testData,aes(x = pos, y = `Per Capita`), color = "blue")
```
